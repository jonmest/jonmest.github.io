[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Cavallie Mester",
    "section": "",
    "text": "Gotta Go Fast: Speeding Up Python With Numba\n\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nJon Cavallie Mester\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "",
    "text": "Python’s charm lies in its simplicity and vibrant ecosystem. But let’s face it: it’s not exactly known for blazing speed. Still, we cling to it, thanks to its ease of use. Sometimes, though, we yearn for both Python’s simplicity and the speed of an F1 car.\nEnter Numba, your Python speedster. It’s not a magic wand, but it’s as close as you’ll get to one in the world of Python."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html#the-quest-for-speed",
    "href": "posts/post-with-code/index.html#the-quest-for-speed",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "The Quest for Speed",
    "text": "The Quest for Speed\nPython, especially CPython, its most common implementation, is an interpreter. This means it has to do some backstage work before executing your code. It compiles your Python script into bytecode and then gets down to business. While this design choice makes Python accessible and portable, it’s not winning any speed contests.\nThen there’s the Global Interpreter Lock (GIL), which makes sure that only one processor thread is in control at any given time. So, even if you have a CPU that’s the envy of your tech-savvy friends, Python might not make the most of it because of the GIL.\nNumba is here to save the day by tossing the GIL aside and turning your Python script into machine code. It pulls off this magic trick through a technique known as Just-In-Time (JIT) compilation."
  },
  {
    "objectID": "posts/post-with-code/index.html#jit-vs.-aot-compilation-the-showdown",
    "href": "posts/post-with-code/index.html#jit-vs.-aot-compilation-the-showdown",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "JIT vs. AOT Compilation: The Showdown",
    "text": "JIT vs. AOT Compilation: The Showdown\nBefore we go any further, let’s get theoretical for a moment. Ahead-Of-Time (AOT) compilation is like baking a cake and serving it right away. You write your program, compile it into a binary, and voila! It’s precompiled and ready to roll. But Just-In-Time (JIT) compilation is a bit different. You write your code, distribute it, and the first time your code gets called, it’s compiled on the spot. The second time and beyond, it’s already compiled and running at full throttle.\nWhy go JIT instead of AOT? Well, for one, you can skip the headache of compiling your code for every platform out there. Plus, you get a speed boost, although there’s a small price to pay during the first code invocation when it’s getting warmed up.\nNow, let’s dig into some code to show you what I’m talking about. To follow along, just pip install numba.\n\nimport numba as nb\n\n@nb.njit\ndef square(x):\n    return x ** 2\n\nsquare(5)\n\n25\nA simple declaration of a Numba JIT-compiled function.\n\n\nAdmittedly, this is a pretty simple example, and you won’t see your code break the sound barrier here. But what’s important is that @nb.njit decorator. It’s the secret sauce that makes your code faster by completely compiling the function to machine code. It gives you the fastest code,but does not work with all Python code and libraries which we’ll discuss later."
  },
  {
    "objectID": "posts/post-with-code/index.html#speed-showdown",
    "href": "posts/post-with-code/index.html#speed-showdown",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "Speed Showdown",
    "text": "Speed Showdown\nBut enough with the theory. How much faster are we talking? To find out, we’ll use the %timeit magic function in our notebook. It runs the code multiple times and gives us the average execution time, which we want since the very first call of our Numba function will trigger a compilation throwing off any single time measurement. We’ll also wrap the regular Python squaring operation in a function to factor in any function call overhead.\n\n%timeit square(5)\n\n124 ns ± 7.13 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n\n\n\ndef standard_square(x):\n    return x ** 2\n%timeit standard_square(5)\n\n180 ns ± 4.09 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n\n\nThe Numba-compiled function is a smidge faster, but we’re not breaking out the champagne just yet. But that’s expected as most of the time’s likely spent in CPython’s function call mechanism and not our tiny piece of compiled code."
  },
  {
    "objectID": "posts/post-with-code/index.html#taking-on-the-big-leagues",
    "href": "posts/post-with-code/index.html#taking-on-the-big-leagues",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "Taking on the Big Leagues",
    "text": "Taking on the Big Leagues\nNow, let’s tackle a real-world example by using Pandas and Numpy to compute a 10-period Simple Moving Average (SMA):\n\nimport numpy as np\nimport pandas as pd\n\ndef pandas_sma(df):\n    return df.rolling(10).mean()\n\narr = np.random.rand(1000)\ndf = pd.DataFrame({\"my_column\": arr})\npandas_sma_result = pandas_sma(df)\n\n%timeit pandas_sma(df)\n\n126 µs ± 769 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\ndef numpy_sma(a, n=10):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\nnumpy_sma_result = numpy_sma(arr)\n%timeit numpy_sma(arr)\n\n8.05 µs ± 66.2 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nWe want to compare them with our own Numba-function written for the same purpose:\n\nimport numba as nb\n\n@nb.njit\ndef numba_sma(arr):\n    window = 10\n    out = np.empty(arr.shape)\n    out[:] = np.nan\n    n = len(arr)\n    for i in range(window, n+1):\n        window_sum = np.sum(arr[i-window:i])\n        out[i-1] = window_sum / window\n    return out\n\nnumba_sma_result = numba_sma(arr)\n%timeit numba_sma(arr)\n\n4.84 µs ± 99.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nBut before we continue, let’s ensure that the Numba and Pandas implementations return similar results:\n\npandas_result_arr = pandas_sma_result[\"my_column\"].values\n\nprint(pandas_result_arr[-5:])\nprint(numba_sma_result[-5:])\nnp.allclose(pandas_result_arr, numba_sma_result, equal_nan=True)\n\n[0.52667708 0.4809015  0.55710991 0.57078993 0.53008837]\n[0.52667708 0.4809015  0.55710991 0.57078993 0.53008837]\n\n\nTrue\nEnsure the results are the same. np.allclose is a great method for checking closeness of values in an array while ignoring small discrepancies.\n\n\nNumba leaves Pandas in the dust, but it’s not even twice as fast Numpy. Why? Because I’m just messing with you, silly! Wakey, wakey!\nWe’re just getting started."
  },
  {
    "objectID": "posts/post-with-code/index.html#the-need-for-speed-reloaded",
    "href": "posts/post-with-code/index.html#the-need-for-speed-reloaded",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "The Need for Speed: Reloaded",
    "text": "The Need for Speed: Reloaded\nWe will crank things up a notch and write an even faster Numba implementation. But it means stepping out of our cozy Pythonic comfort zone and diving headfirst into regular for-loops and verbose code:\n\n@nb.njit\ndef numba_sma_faster(arr):\n    window_size = 10\n    n = len(arr)\n    out = np.empty(n)\n    out[:] = np.nan\n    accumulator = 0.0\n\n    for i in range(n):\n        if i &lt; window_size - 1:\n            accumulator += arr[i]\n            out[i] = np.nan\n        else:\n            accumulator += arr[i]\n            if i &gt;= window_size:\n                accumulator -= arr[i - window_size]\n            out[i] = accumulator / window_size\n\n    return out\n\nfaster_numba_sma_result = numba_sma_faster(arr)\n%timeit numba_sma_faster(arr)\n\n2.34 µs ± 23.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nnp.allclose(pandas_result_arr, faster_numba_sma_result, equal_nan=True)\n\nTrue\n\n\nWe’re using the accumulator pattern here, which is much more efficient. Instead of repeatedly accessing all items in the window to compute the sum, we maintain a running total (the accumulator) and subtract the value of the item exiting the window while adding the value of the item entering the window.\nIn traditional Python, this approach is not the best, and it would be slower than Numpy. But in Numba-land, these blasphemous for-loops can be blazing fast. This function is over 50 times faster than Pandas, over 3 times faster than Numpy, and 2 times faster than the previous Numba version. It may seem a bit unpythonic, but the results speak for themselves.\nNow, let’s dive into the world of parallelization. I mentioned earlier that the Global Interpreter Lock (GIL) in CPython can be a buzzkill when you’re trying to parallelize Python programs. But guess what? Numba, once again, comes to the rescue.\nWhen we use Numba’s njit decorator, the code in the JIT-compiled function isn’t interpreted; it’s already compiled into machine code. This means we can sidestep the GIL entirely! In fact, Numba can even automatically parallelize array operations if you use the parallel keyword argument. Then you have numba.prange. It’s like the regular Python range, but with a twist. If parallelization is enabled, the loop executes in parallel. If it’s disabled, it behaves just like its standard counterpart.\nNow, let’s put this to the test with our earlier, slightly sluggish Numba SMA function, and a much larger array since it does not make sense to spawn new threads for an operation that only takes a few microseconds in total:\n\n@nb.njit(parallel=True)\ndef numba_sma_parallel(arr):\n    window = 10\n    out = np.empty(arr.shape)\n    out[:] = np.nan\n    n = len(arr)\n    for i in nb.prange(window, n+1):\n        window_sum = np.sum(arr[i-window:i])\n        out[i-1] = window_sum / window\n    return out\n\narr = np.random.rand(1_000_000)\nnumba_sma_parallel_result = numba_sma_parallel(arr)\n%timeit numba_sma_parallel(arr)\n\n800 µs ± 14.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nCompare with Numpy:\n\nnumpy_sma_result = numpy_sma(arr)\n%timeit numpy_sma(arr)\n\n3.73 ms ± 122 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nOver three times faster than the Numpy variant, with barely any changes! I’d say that’s impressive."
  },
  {
    "objectID": "posts/post-with-code/index.html#the-pitfalls-of-numba",
    "href": "posts/post-with-code/index.html#the-pitfalls-of-numba",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "The Pitfalls Of Numba",
    "text": "The Pitfalls Of Numba\nWe’ve barely scratched the surface of how great Numba is, but it’s getting late and I need time to tell you about what to watch out for when working with Numba.\nIf you paid attention (you did, right?) we’ve exclusively worked with Numba’s njit decorator which fully compiles the decorated function into machine code and is the recommended way to go. @njit is actually shorthand for another Numba decorator written like @jit(nopython=True). As you might deduce, it’s activating some sort of nopython Numba mode.\nIf you don’t activate nopython mode, Numba will fall back to object mode which looks for loops that can be compiled and leaves the rest as interpreted Python code. If it’s unable to compile any of the code, it will fail silently and just perform just as slow as any other piece of CPython. You get it, it’s not great. But there is a use case when you are unable to use nopython mode and object mode is the best you can get. For example, some functions can’t be compiled:\n\nimport numba as nb\nfrom scipy.stats import entropy\nimport numpy as np\n\n@nb.njit\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\ntry:\n    entropy_nb(np.array([0.5, 0.5]))\nexcept nb.TypingError as e:\n    print(e)\n\nFailed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'entropy': Cannot determine Numba type of &lt;class 'function'&gt;\n\nFile \"../../../../../../tmp/ipykernel_179513/538693046.py\", line 7:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n\nThat’s some ugly typing error. Sadly, a lot of SciPy functions can’t be used in nopython mode partly because there are some C-bindings that don’t play too well. Let’s try without nopython mode:\n\nimport numba as nb\nfrom scipy.stats import entropy\nimport numpy as np\n\n@nb.jit # ← Notice \"jit\" instead of \"njit\"\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n\n/tmp/ipykernel_179513/3177759806.py:6: NumbaDeprecationWarning:\n\nThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n\n/tmp/ipykernel_179513/3177759806.py:5: NumbaWarning:\n\n\nCompilation is falling back to object mode WITH looplifting enabled because Function \"entropy_nb\" failed type inference due to: Untyped global name 'entropy': Cannot determine Numba type of &lt;class 'function'&gt;\n\nFile \"../../../../../../tmp/ipykernel_179513/3177759806.py\", line 7:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning:\n\nFunction \"entropy_nb\" was compiled in object mode without forceobj=True.\n\nFile \"../../../../../../tmp/ipykernel_179513/3177759806.py\", line 5:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning:\n\n\nFall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n\nFile \"../../../../../../tmp/ipykernel_179513/3177759806.py\", line 5:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n\n\n1.0\n\n\nWe get some warnings that Numba tries to compile the function, fails and falls back on object mode. We also get the warning that the fallback behavior will be removed in Numba 0.59.0, so let’s also try the now recommended way to go object mode.\n\nimport numba as nb\nfrom scipy.stats import entropy\nimport numpy as np\n\n@nb.jit(forceobj=True)\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n\n1.0\n\n\nNo warnings, no compilation, but it just works. Of course, it’s pointless to even use Numba in this example as it won’t do anything at all. Where it might be useful is in a larger function where some parts can’t be compiled but there are loops that can and should."
  },
  {
    "objectID": "posts/post-with-code/index.html#navigating-the-numba-maze-pitfalls-to-watch-out-for",
    "href": "posts/post-with-code/index.html#navigating-the-numba-maze-pitfalls-to-watch-out-for",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "Navigating the Numba Maze: Pitfalls to Watch Out For",
    "text": "Navigating the Numba Maze: Pitfalls to Watch Out For\nWe’ve barely scratched the surface of Numba’s greatness, but before we call it a day, it’s crucial to discuss the potential pitfalls that come with the territory of working with Numba.\nNow, if you’ve been paying attention (and I trust you have), we’ve exclusively danced with Numba’s njit decorator, which, let’s be clear, is the gold standard. @njit is essentially a shorthand notation for another Numba decorator, expressed as @jit(nopython=True). You see, it’s like flipping a switch into what Numba aficionados call the nopython mode where all the code is compiled.\nThere’s also an object mode which Numba falls back on if Numba is unable to compile all of the code. In this mode, Numba becomes a bit of a detective, looking for loops it can convert into faster machine code. However, the rest of your code remains as interpreted Python code. If you don’t enforce nopython mode using @njit or jit(nopython=True), Numba might fall back to object mode, log some warnings, and provide little to no performance benefits at all.\nBut there are scenarios where you’re practically forced to abandon the cozy realm of nopython mode, and go with object object mode as the second best option.\nLook here:\n\nfrom scipy.stats import entropy\n\n@nb.njit\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\ntry:\n    entropy_nb(np.array([0.5, 0.5]))\nexcept nb.TypingError as e:\n    print(e)\n\nFailed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'entropy': Cannot determine Numba type of &lt;class 'function'&gt;\n\nFile \"../../../../../../tmp/ipykernel_181160/4199264108.py\", line 5:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n\nThat’s some ugly typing error. A lot of SciPy functions can’t be used in nopython mode because they are C-bindings that don’t play too well with Numba. Let’s try it without nopython mode enforced:\n\n@nb.jit # ← Notice \"jit\" instead of \"njit\"\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n\n/tmp/ipykernel_181160/526531211.py:2: NumbaDeprecationWarning:\n\nThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n\n/tmp/ipykernel_181160/526531211.py:1: NumbaWarning:\n\n\nCompilation is falling back to object mode WITH looplifting enabled because Function \"entropy_nb\" failed type inference due to: Untyped global name 'entropy': Cannot determine Numba type of &lt;class 'function'&gt;\n\nFile \"../../../../../../tmp/ipykernel_181160/526531211.py\", line 3:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning:\n\nFunction \"entropy_nb\" was compiled in object mode without forceobj=True.\n\nFile \"../../../../../../tmp/ipykernel_181160/526531211.py\", line 1:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning:\n\n\nFall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n\nFile \"../../../../../../tmp/ipykernel_181160/526531211.py\", line 1:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n\n\n1.0\n\n\nSome pesky warnings pop up, Numba attempts to compile, realizes it’s in over its head, and gracefully falls back to object mode. In the end, it works as expected, but it does not exactly look pretty, does it?\nBefore we part ways, let’s heed Numba’s advice in the logged warnings and explicitly enforce object mode:\n\n@nb.jit(forceobj=True)\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n\n1.0\n\n\nNo warnings, no fancy compilation attempts - it just works. Of course, in this particular example, using Numba might seem a bit like bringing a rocket launcher to a thumb wrestling match, as it won’t provide any significant performance boost. But where it might shine is in a more complex function where some parts can’t be compiled, yet there are loops that could certainly benefit from Numba’s nopython mode."
  },
  {
    "objectID": "posts/post-with-code/index.html#conclusion",
    "href": "posts/post-with-code/index.html#conclusion",
    "title": "Got to go fast: Speeding up Python with Numba",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, Numba is a powerful tool for enhancing the performance of Python code. Its ability to compile Python functions into machine code using JIT compilation can significantly accelerate computations. We explored its advantages, from basic usage to more complex scenarios like parallelization.\nHowever, it’s essential to be aware of potential pitfalls, such as falling back to “object” mode when “nopython” mode isn’t feasible, particularly for certain external libraries or complex functions. While Numba provides an effective means to optimize Python, careful consideration of its application is necessary to maximize its benefits.\nIf this post piqued your interest, I recommend that you connect with me on LinkedIn and check out Numba’s documentation. Get in touch if you have any questions."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html",
    "href": "posts/numba-python-jit-introduction/index.html",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "",
    "text": "Python’s charm lies in its simplicity and vibrant ecosystem. But let’s face it: it’s not exactly known for blazing speed. Still, we cling to it, thanks to its ease of use. Sometimes, though, we yearn for both Python’s simplicity and the speed of an F1 car.\nEnter Numba, your Python speedster. It’s not a magic wand, but it’s as close as you’ll get to one in the world of Python."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#the-quest-for-speed",
    "href": "posts/numba-python-jit-introduction/index.html#the-quest-for-speed",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "The Quest for Speed",
    "text": "The Quest for Speed\nPython, especially CPython, its most common implementation, is an interpreter. This means it has to do some backstage work before executing your code. It compiles your Python script into bytecode and then gets down to business. While this design choice makes Python accessible and portable, it’s not winning any speed contests.\nThen there’s the Global Interpreter Lock (GIL), which makes sure that only one processor thread is in control at any given time. So, even if you have a CPU that’s the envy of your tech-savvy friends, Python might not make the most of it because of the GIL.\nNumba is here to save the day by tossing the GIL aside and turning your Python script into machine code. It pulls off this magic trick through a technique known as Just-In-Time (JIT) compilation."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#jit-vs.-aot-compilation-the-showdown",
    "href": "posts/numba-python-jit-introduction/index.html#jit-vs.-aot-compilation-the-showdown",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "JIT vs. AOT Compilation: The Showdown",
    "text": "JIT vs. AOT Compilation: The Showdown\nBefore we go any further, let’s get theoretical for a moment. Ahead-Of-Time (AOT) compilation is like baking a cake and serving it right away. You write your program, compile it into a binary, and voila! It’s precompiled and ready to roll. But Just-In-Time (JIT) compilation is a bit different. You write your code, distribute it, and the first time your code gets called, it’s compiled on the spot. The second time and beyond, it’s already compiled and running at full throttle.\nWhy go JIT instead of AOT? Well, for one, you can skip the headache of compiling your code for every platform out there. Plus, you get a speed boost, although there’s a small price to pay during the first code invocation when it’s getting warmed up.\nNow, let’s dig into some code to show you what I’m talking about. To follow along, just pip install numba.\n\nimport numba as nb\n\n@nb.njit\ndef square(x):\n    return x ** 2\n\nsquare(5)\n\n25\nA simple declaration of a Numba JIT-compiled function.\n\n\nAdmittedly, this is a pretty simple example, and you won’t see your code break the sound barrier here. But what’s important is that @nb.njit decorator. It’s the secret sauce that makes your code faster by completely compiling the function to machine code. It gives you the fastest code,but does not work with all Python code and libraries which we’ll discuss later."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#speed-showdown",
    "href": "posts/numba-python-jit-introduction/index.html#speed-showdown",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "Speed Showdown",
    "text": "Speed Showdown\nBut enough with the theory. How much faster are we talking? To find out, we’ll use the %timeit magic function in our notebook. It runs the code multiple times and gives us the average execution time, which we want since the very first call of our Numba function will trigger a compilation throwing off any single time measurement. We’ll also wrap the regular Python squaring operation in a function to factor in any function call overhead.\n\n%timeit square(5)\n\n116 ns ± 3.34 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n\n\n\ndef standard_square(x):\n    return x ** 2\n%timeit standard_square(5)\n\n204 ns ± 22.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe Numba-compiled function is a smidge faster, but we’re not breaking out the champagne just yet. But that’s expected as most of the time’s likely spent in CPython’s function call mechanism and not our tiny piece of compiled code."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#taking-on-the-big-leagues",
    "href": "posts/numba-python-jit-introduction/index.html#taking-on-the-big-leagues",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "Taking on the Big Leagues",
    "text": "Taking on the Big Leagues\nNow, let’s tackle a real-world example by using Pandas and Numpy to compute a 10-period Simple Moving Average (SMA):\n\nimport numpy as np\nimport pandas as pd\n\ndef pandas_sma(df):\n    return df.rolling(10).mean()\n\narr = np.random.rand(1000)\ndf = pd.DataFrame({\"my_column\": arr})\npandas_sma_result = pandas_sma(df)\n\n%timeit pandas_sma(df)\n\n131 µs ± 1.77 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\ndef numpy_sma(a, n=10):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\nnumpy_sma_result = numpy_sma(arr)\n%timeit numpy_sma(arr)\n\n7.52 µs ± 66.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nWe want to compare them with our own Numba-function written for the same purpose:\n\nimport numba as nb\n\n@nb.njit\ndef numba_sma(arr):\n    window = 10\n    out = np.empty(arr.shape)\n    out[:] = np.nan\n    n = len(arr)\n    for i in range(window, n+1):\n        window_sum = np.sum(arr[i-window:i])\n        out[i-1] = window_sum / window\n    return out\n\nnumba_sma_result = numba_sma(arr)\n%timeit numba_sma(arr)\n\n4.69 µs ± 22.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nBut before we continue, let’s ensure that the Numba and Pandas implementations return similar results:\n\npandas_result_arr = pandas_sma_result[\"my_column\"].values\n\nprint(pandas_result_arr[-5:])\nprint(numba_sma_result[-5:])\nnp.allclose(pandas_result_arr, numba_sma_result, equal_nan=True)\n\n[0.45720976 0.41700846 0.4315914  0.4301416  0.42904101]\n[0.45720976 0.41700846 0.4315914  0.4301416  0.42904101]\n\n\nTrue\nEnsure the results are the same. np.allclose is a great method for checking closeness of values in an array while ignoring small discrepancies.\n\n\nNumba leaves Pandas in the dust, but it’s not even twice as fast Numpy. Why? Because I’m just messing with you, silly! Wakey, wakey!\nWe’re just getting started."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#the-need-for-speed-reloaded",
    "href": "posts/numba-python-jit-introduction/index.html#the-need-for-speed-reloaded",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "The Need for Speed: Reloaded",
    "text": "The Need for Speed: Reloaded\nWe will crank things up a notch and write an even faster Numba implementation. But it means stepping out of our cozy Pythonic comfort zone and diving headfirst into regular for-loops and verbose code:\n\n@nb.njit\ndef numba_sma_faster(arr):\n    window_size = 10\n    n = len(arr)\n    out = np.empty(n)\n    out[:] = np.nan\n    accumulator = 0.0\n\n    for i in range(n):\n        if i &lt; window_size - 1:\n            accumulator += arr[i]\n            out[i] = np.nan\n        else:\n            accumulator += arr[i]\n            if i &gt;= window_size:\n                accumulator -= arr[i - window_size]\n            out[i] = accumulator / window_size\n\n    return out\n\nfaster_numba_sma_result = numba_sma_faster(arr)\n%timeit numba_sma_faster(arr)\n\n2.3 µs ± 6.09 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nnp.allclose(pandas_result_arr, faster_numba_sma_result, equal_nan=True)\n\nTrue\n\n\nWe’re using the accumulator pattern here, which is much more efficient. Instead of repeatedly accessing all items in the window to compute the sum, we maintain a running total (the accumulator) and subtract the value of the item exiting the window while adding the value of the item entering the window.\nIn traditional Python, this approach is not the best, and it would be slower than Numpy. But in Numba-land, these blasphemous for-loops can be blazing fast. This function is over 50 times faster than Pandas, over 3 times faster than Numpy, and 2 times faster than the previous Numba version. It may seem a bit unpythonic, but the results speak for themselves."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#nice-but-can-it-run-parallel",
    "href": "posts/numba-python-jit-introduction/index.html#nice-but-can-it-run-parallel",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "Nice, but can it run parallel?",
    "text": "Nice, but can it run parallel?\nNow, let’s dive into the world of parallelization. I mentioned earlier that the Global Interpreter Lock (GIL) in CPython can be a buzzkill when you’re trying to parallelize Python programs. But guess what? Numba, once again, comes to the rescue.\nWhen we use Numba’s njit decorator, the code in the JIT-compiled function isn’t interpreted; it’s already compiled into machine code. This means we can sidestep the GIL entirely! In fact, Numba can even automatically parallelize array operations if you use the parallel keyword argument. Then you have numba.prange. It’s like the regular Python range, but with a twist. If parallelization is enabled, the loop executes in parallel. If it’s disabled, it behaves just like its standard counterpart.\nNow, let’s put this to the test with our earlier, slightly sluggish Numba SMA function, and a much larger array since it does not make sense to spawn new threads for an operation that only takes a few microseconds in total:\n\n@nb.njit(parallel=True)\ndef numba_sma_parallel(arr):\n    window = 10\n    out = np.empty(arr.shape)\n    out[:] = np.nan\n    n = len(arr)\n    for i in nb.prange(window, n+1):\n        window_sum = np.sum(arr[i-window:i])\n        out[i-1] = window_sum / window\n    return out\n\narr = np.random.rand(1_000_000)\nnumba_sma_parallel_result = numba_sma_parallel(arr)\n%timeit numba_sma_parallel(arr)\n\n778 µs ± 18.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nCompare with Numpy:\n\nnumpy_sma_result = numpy_sma(arr)\n%timeit numpy_sma(arr)\n\n3.39 ms ± 17.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nOver three times faster than the Numpy variant, with barely any changes! I’d say that’s impressive."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#navigating-the-numba-maze-pitfalls-to-watch-out-for",
    "href": "posts/numba-python-jit-introduction/index.html#navigating-the-numba-maze-pitfalls-to-watch-out-for",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "Navigating the Numba Maze: Pitfalls to Watch Out For",
    "text": "Navigating the Numba Maze: Pitfalls to Watch Out For\nWe’ve barely scratched the surface of Numba’s greatness, but before we call it a day, it’s crucial to discuss the potential pitfalls that come with the territory of working with Numba.\nNow, if you’ve been paying attention (and I trust you have), we’ve exclusively danced with Numba’s njit decorator, which, let’s be clear, is the gold standard. @njit is essentially a shorthand notation for another Numba decorator, expressed as @jit(nopython=True). You see, it’s like flipping a switch into what Numba aficionados call the nopython mode where all the code is compiled.\nThere’s also an object mode which Numba falls back on if Numba is unable to compile all of the code. In this mode, Numba becomes a bit of a detective, looking for loops it can convert into faster machine code. However, the rest of your code remains as interpreted Python code. If you don’t enforce nopython mode using @njit or jit(nopython=True), Numba might fall back to object mode, log some warnings, and provide little to no performance benefits at all.\nBut there are scenarios where you’re practically forced to abandon the cozy realm of nopython mode, and go with object object mode as the second best option.\nLook here:\n\nfrom scipy.stats import entropy\n\n@nb.njit\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\ntry:\n    entropy_nb(np.array([0.5, 0.5]))\nexcept nb.TypingError as e:\n    print(e)\n\nFailed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'entropy': Cannot determine Numba type of &lt;class 'function'&gt;\n\nFile \"../../../../../../tmp/ipykernel_183868/4199264108.py\", line 5:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n\nThat’s some ugly typing error. A lot of SciPy functions can’t be used in nopython mode because they are C-bindings that don’t play too well with Numba. Let’s try it without nopython mode enforced:\n\n@nb.jit # ← Notice \"jit\" instead of \"njit\"\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n\n/tmp/ipykernel_183868/526531211.py:2: NumbaDeprecationWarning:\n\nThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n\n/tmp/ipykernel_183868/526531211.py:1: NumbaWarning:\n\n\nCompilation is falling back to object mode WITH looplifting enabled because Function \"entropy_nb\" failed type inference due to: Untyped global name 'entropy': Cannot determine Numba type of &lt;class 'function'&gt;\n\nFile \"../../../../../../tmp/ipykernel_183868/526531211.py\", line 3:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning:\n\nFunction \"entropy_nb\" was compiled in object mode without forceobj=True.\n\nFile \"../../../../../../tmp/ipykernel_183868/526531211.py\", line 1:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning:\n\n\nFall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n\nFile \"../../../../../../tmp/ipykernel_183868/526531211.py\", line 1:\n&lt;source missing, REPL/exec in use?&gt;\n\n\n\n\n1.0\n\n\nSome pesky warnings pop up, Numba attempts to compile, realizes it’s in over its head, and gracefully falls back to object mode. In the end, it works as expected, but it does not exactly look pretty, does it?\nBefore we part ways, let’s heed Numba’s advice in the logged warnings and explicitly enforce object mode:\n\n@nb.jit(forceobj=True)\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n\n1.0\n\n\nNo warnings, no fancy compilation attempts - it just works. Of course, in this particular example, using Numba might seem a bit like bringing a rocket launcher to a thumb wrestling match, as it won’t provide any significant performance boost. But where it might shine is in a more complex function where some parts can’t be compiled, yet there are loops that could certainly benefit from Numba’s nopython mode."
  },
  {
    "objectID": "posts/numba-python-jit-introduction/index.html#conclusion",
    "href": "posts/numba-python-jit-introduction/index.html#conclusion",
    "title": "Gotta Go Fast: Speeding Up Python With Numba",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, Numba is a powerful tool for enhancing the performance of Python code. Its ability to compile Python functions into machine code using JIT compilation can significantly accelerate computations. We explored its advantages, from basic usage to more complex scenarios like parallelization.\nHowever, it’s essential to be aware of potential pitfalls, such as falling back to “object” mode when “nopython” mode isn’t feasible, particularly for certain external libraries or complex functions. While Numba provides an effective means to optimize Python, careful consideration of its application is necessary to maximize its benefits.\nIf this post piqued your interest, I recommend that you connect with me on LinkedIn and check out Numba’s documentation. Get in touch if you have any questions."
  }
]