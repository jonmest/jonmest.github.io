{
  "hash": "fe4db7ec9b8836906fa96b63456cb5a3",
  "result": {
    "markdown": "---\ntitle: 'Gotta Go Fast: Speeding Up Python With Numba'\nauthor: Jon Cavallie Mester\ndate: '2023-09-04'\nformat:\n  html:\n    code-fold: false\n---\n\nPython's charm lies in its simplicity and vibrant ecosystem. But let's face it: it's not exactly known for blazing speed. Still, we cling to it, thanks to its ease of use. Sometimes, though, we yearn for both Python's simplicity and the speed of an F1 car.\n\nEnter Numba, your Python speedster. It's not a magic wand, but it's as close as you'll get to one in the world of Python.\n\n## The Quest for Speed\n\nPython, especially CPython, its most common implementation, is an interpreter. This means it has to do some backstage work before executing your code. It compiles your Python script into bytecode and then gets down to business. While this design choice makes Python accessible and portable, it's not winning any speed contests.\n\nThen there's the Global Interpreter Lock (GIL), which makes sure that only one processor thread is in control at any given time. So, even if you have a CPU that's the envy of your tech-savvy friends, Python might not make the most of it because of the GIL.\n\nNumba is here to save the day by tossing the GIL aside and turning your Python script into machine code. It pulls off this magic trick through a technique known as Just-In-Time (JIT) compilation.\n\n## JIT vs. AOT Compilation: The Showdown\n\nBefore we go any further, let's get theoretical for a moment. Ahead-Of-Time (AOT) compilation is like baking a cake and serving it right away. You write your program, compile it into a binary, and voila! It's precompiled and ready to roll. But Just-In-Time (JIT) compilation is a bit different. You write your code, distribute it, and the first time your code gets called, it's compiled on the spot. The second time and beyond, it's already compiled and running at full throttle.\n\nWhy go JIT instead of AOT? Well, for one, you can skip the headache of compiling your code for every platform out there. Plus, you get a speed boost, although there's a small price to pay during the first code invocation when it's getting warmed up.\n\nNow, let's dig into some code to show you what I'm talking about. To follow along, just `pip install numba`.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numba as nb\n\n@nb.njit\ndef square(x):\n    return x ** 2\n\nsquare(5)\n```\n\n::: {#basic-numba-demo .cell-output .cell-output-display execution_count=1}\n```\n25\n```\n\nA simple declaration of a Numba JIT-compiled function.\n:::\n:::\n\n\nAdmittedly, this is a pretty simple example, and you won't see your code break the sound barrier here. But what's important is that `@nb.njit` decorator. It's the secret sauce that makes your code faster by *completely* compiling the function to machine code. It gives you the fastest code,but does not work with all Python code and libraries which we'll discuss later.\n\n## Speed Showdown\n\nBut enough with the theory. How much faster are we talking? To find out, we'll use the `%timeit` magic function in our notebook. It runs the code multiple times and gives us the average execution time, which we want since the very first call of our Numba function will trigger a compilation throwing off any single time measurement. We'll also wrap the regular Python squaring operation in a function to factor in any function call overhead.\n\n::: {#basic-numba-demo-speed .cell execution_count=2}\n``` {.python .cell-code}\n%timeit square(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n116 ns ± 3.34 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n```\n:::\n:::\n\n\n::: {#basic-python-speed .cell execution_count=3}\n``` {.python .cell-code}\ndef standard_square(x):\n    return x ** 2\n%timeit standard_square(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n204 ns ± 22.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n```\n:::\n:::\n\n\nThe Numba-compiled function is a smidge faster, but we're not breaking out the champagne just yet. But that's expected as most of the time's likely spent in CPython's function call mechanism and not our tiny piece of compiled code.\n\n## Taking on the Big Leagues\n\nNow, let's tackle a real-world example by using Pandas and Numpy to compute a 10-period Simple Moving Average (SMA):\n\n::: {#basic-pandas-speed .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\ndef pandas_sma(df):\n    return df.rolling(10).mean()\n\narr = np.random.rand(1000)\ndf = pd.DataFrame({\"my_column\": arr})\npandas_sma_result = pandas_sma(df)\n\n%timeit pandas_sma(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n131 µs ± 1.77 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n```\n:::\n:::\n\n\n::: {#basic-numpy-sma-speed .cell execution_count=5}\n``` {.python .cell-code}\ndef numpy_sma(a, n=10):\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\nnumpy_sma_result = numpy_sma(arr)\n%timeit numpy_sma(arr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n7.52 µs ± 66.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n```\n:::\n:::\n\n\nWe want to compare them with our own Numba-function written for the same purpose:\n\n::: {#basic-numba-sma-speed .cell execution_count=6}\n``` {.python .cell-code}\nimport numba as nb\n\n@nb.njit\ndef numba_sma(arr):\n    window = 10\n    out = np.empty(arr.shape)\n    out[:] = np.nan\n    n = len(arr)\n    for i in range(window, n+1):\n        window_sum = np.sum(arr[i-window:i])\n        out[i-1] = window_sum / window\n    return out\n\nnumba_sma_result = numba_sma(arr)\n%timeit numba_sma(arr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.69 µs ± 22.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n```\n:::\n:::\n\n\nBut before we continue, let's ensure that the Numba and Pandas implementations return similar results:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\npandas_result_arr = pandas_sma_result[\"my_column\"].values\n\nprint(pandas_result_arr[-5:])\nprint(numba_sma_result[-5:])\nnp.allclose(pandas_result_arr, numba_sma_result, equal_nan=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0.45720976 0.41700846 0.4315914  0.4301416  0.42904101]\n[0.45720976 0.41700846 0.4315914  0.4301416  0.42904101]\n```\n:::\n\n::: {#compare-outputs .cell-output .cell-output-display execution_count=7}\n```\nTrue\n```\n\nEnsure the results are the same. np.allclose is a great method for checking closeness of values in an array while ignoring small discrepancies.\n:::\n:::\n\n\nNumba leaves Pandas in the dust, but it's not even twice as fast Numpy. Why? Because I'm just messing with you, silly! Wakey, wakey! \n\nWe're just getting started.\n\n## The Need for Speed: Reloaded\n\nWe will crank things up a notch and write an even faster Numba implementation. But it means stepping out of our cozy Pythonic comfort zone and diving headfirst into regular for-loops and verbose code:\n\n::: {#faster-numba-sma .cell execution_count=8}\n``` {.python .cell-code}\n@nb.njit\ndef numba_sma_faster(arr):\n    window_size = 10\n    n = len(arr)\n    out = np.empty(n)\n    out[:] = np.nan\n    accumulator = 0.0\n\n    for i in range(n):\n        if i < window_size - 1:\n            accumulator += arr[i]\n            out[i] = np.nan\n        else:\n            accumulator += arr[i]\n            if i >= window_size:\n                accumulator -= arr[i - window_size]\n            out[i] = accumulator / window_size\n\n    return out\n\nfaster_numba_sma_result = numba_sma_faster(arr)\n%timeit numba_sma_faster(arr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.3 µs ± 6.09 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nnp.allclose(pandas_result_arr, faster_numba_sma_result, equal_nan=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nTrue\n```\n:::\n:::\n\n\nWe're using the accumulator pattern here, which is much more efficient. Instead of repeatedly accessing all items in the window to compute the sum, we maintain a running total (the accumulator) and subtract the value of the item exiting the window while adding the value of the item entering the window.\n\nIn traditional Python, this approach is not the best, and it would be slower than Numpy. But in Numba-land, these blasphemous for-loops can be blazing fast. This function is over 50 times faster than Pandas, over 3 times faster than Numpy, and 2 times faster than the previous Numba version. It may seem a bit unpythonic, but the results speak for themselves.\n\n## Nice, but can it run parallel?\n\nNow, let's dive into the world of parallelization. I mentioned earlier that the Global Interpreter Lock (GIL) in CPython can be a buzzkill when you're trying to parallelize Python programs. But guess what? Numba, once again, comes to the rescue.\n\nWhen we use Numba's `njit` decorator, the code in the JIT-compiled function isn't interpreted; it's already compiled into machine code. This means we can sidestep the GIL entirely! In fact, Numba can even automatically parallelize array operations if you use the `parallel` keyword argument. Then you have `numba.prange`. It's like the regular Python `range`, but with a twist. If parallelization is enabled, the loop executes in parallel. If it's disabled, it behaves just like its standard counterpart.\n\nNow, let's put this to the test with our earlier, slightly sluggish Numba SMA function, and a much larger array since it does not make sense to spawn new threads for an operation that only takes a few microseconds in total:\n\n::: {#basic-numba-sma-speed-parallel .cell execution_count=10}\n``` {.python .cell-code}\n@nb.njit(parallel=True)\ndef numba_sma_parallel(arr):\n    window = 10\n    out = np.empty(arr.shape)\n    out[:] = np.nan\n    n = len(arr)\n    for i in nb.prange(window, n+1):\n        window_sum = np.sum(arr[i-window:i])\n        out[i-1] = window_sum / window\n    return out\n\narr = np.random.rand(1_000_000)\nnumba_sma_parallel_result = numba_sma_parallel(arr)\n%timeit numba_sma_parallel(arr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n778 µs ± 18.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n```\n:::\n:::\n\n\nCompare with Numpy:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nnumpy_sma_result = numpy_sma(arr)\n%timeit numpy_sma(arr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3.39 ms ± 17.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n```\n:::\n:::\n\n\nOver three times faster than the Numpy variant, with barely any changes! I'd say that's impressive.\n\n## Navigating the Numba Maze: Pitfalls to Watch Out For\nWe've barely scratched the surface of Numba's greatness, but before we call it a day, it's crucial to discuss the potential pitfalls that come with the territory of working with Numba.\n\nNow, if you've been paying attention (and I trust you have), we've exclusively danced with Numba's njit decorator, which, let's be clear, is the gold standard. `@njit` is essentially a shorthand notation for another Numba decorator, expressed as `@jit(nopython=True)`. You see, it's like flipping a switch into what Numba aficionados call the `nopython` mode where all the code is compiled.\n\nThere's also an `object` mode which Numba falls back on if Numba is unable to compile all of the code. In this mode, Numba becomes a bit of a detective, looking for loops it can convert into faster machine code. However, the rest of your code remains as interpreted Python code.  If you don't enforce `nopython` mode using `@njit` or `jit(nopython=True)`, Numba might fall back to `object` mode, log some warnings, and provide little to no performance benefits at all.\n\nBut there are scenarios where you're practically forced to abandon the cozy realm of `nopython` mode, and go with `object` object mode as the second best option.\n\nLook here:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nfrom scipy.stats import entropy\n\n@nb.njit\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\ntry:\n    entropy_nb(np.array([0.5, 0.5]))\nexcept nb.TypingError as e:\n    print(e)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFailed in nopython mode pipeline (step: nopython frontend)\nUntyped global name 'entropy': Cannot determine Numba type of <class 'function'>\n\nFile \"../../../../../../tmp/ipykernel_183868/4199264108.py\", line 5:\n<source missing, REPL/exec in use?>\n\n```\n:::\n:::\n\n\nThat's some ugly typing error. A lot of SciPy functions can't be used in `nopython` mode because they are C-bindings that don't play too well with Numba. Let's try it without `nopython` mode enforced:\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n@nb.jit # ← Notice \"jit\" instead of \"njit\"\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_183868/526531211.py:2: NumbaDeprecationWarning:\n\nThe 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n\n/tmp/ipykernel_183868/526531211.py:1: NumbaWarning:\n\n\nCompilation is falling back to object mode WITH looplifting enabled because Function \"entropy_nb\" failed type inference due to: Untyped global name 'entropy': Cannot determine Numba type of <class 'function'>\n\nFile \"../../../../../../tmp/ipykernel_183868/526531211.py\", line 3:\n<source missing, REPL/exec in use?>\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:151: NumbaWarning:\n\nFunction \"entropy_nb\" was compiled in object mode without forceobj=True.\n\nFile \"../../../../../../tmp/ipykernel_183868/526531211.py\", line 1:\n<source missing, REPL/exec in use?>\n\n\n/home/jon/.local/lib/python3.10/site-packages/numba/core/object_mode_passes.py:161: NumbaDeprecationWarning:\n\n\nFall-back from the nopython compilation path to the object mode compilation path has been detected. This is deprecated behaviour that will be removed in Numba 0.59.0.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit\n\nFile \"../../../../../../tmp/ipykernel_183868/526531211.py\", line 1:\n<source missing, REPL/exec in use?>\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n1.0\n```\n:::\n:::\n\n\nSome pesky warnings pop up, Numba attempts to compile, realizes it's in over its head, and gracefully falls back to object mode. In the end, it works as expected, but it does not exactly look pretty, does it?\n\nBefore we part ways, let's heed Numba's advice in the logged warnings and explicitly enforce `object` mode:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n@nb.jit(forceobj=True)\ndef entropy_nb(probability_distribution):\n    return entropy(probability_distribution, base=2)\n\n# Calculate Shannon entropy for a fair coin toss\nentropy_nb(np.array([0.5, 0.5]))\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n1.0\n```\n:::\n:::\n\n\nNo warnings, no fancy compilation attempts - it just works. Of course, in this particular example, using Numba might seem a bit like bringing a rocket launcher to a thumb wrestling match, as it won't provide any significant performance boost. But where it might shine is in a more complex function where some parts can't be compiled, yet there are loops that could certainly benefit from Numba's `nopython` mode. \n\n## Conclusion\n\nIn conclusion, Numba is a powerful tool for enhancing the performance of Python code. Its ability to compile Python functions into machine code using JIT compilation can significantly accelerate computations. We explored its advantages, from basic usage to more complex scenarios like parallelization.\n\nHowever, it's essential to be aware of potential pitfalls, such as falling back to \"object\" mode when \"nopython\" mode isn't feasible, particularly for certain external libraries or complex functions. While Numba provides an effective means to optimize Python, careful consideration of its application is necessary to maximize its benefits.\n\nIf this post piqued your interest, I recommend that you connect with me on [LinkedIn](https://www.linkedin.com/in/jon-mester/) and check out Numba's [documentation](https://numba.readthedocs.io). Get in touch if you have any questions.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}